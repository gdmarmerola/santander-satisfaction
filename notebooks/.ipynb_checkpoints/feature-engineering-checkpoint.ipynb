{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Santander customer satisfaction: feature engineering\n",
    "\n",
    "Improving data by transforming the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# starting up a console attached to this kernel\n",
    "%matplotlib inline\n",
    "%qtconsole\n",
    "import os\n",
    "\n",
    "# importing base code\n",
    "os.chdir('/home/guilherme/Documents/Kaggle/santander-satisfaction/code')\n",
    "from base import *\n",
    "\n",
    "# changing to competition dir\n",
    "os.chdir('/home/guilherme/Documents/Kaggle/santander-satisfaction')\n",
    "\n",
    "# target variable\n",
    "target = pd.read_csv('data/target.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-sum feature\n",
    "\n",
    "Counting how many columns are equal to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# reading train and test (real)\n",
    "train = pd.read_csv('data/no-duplicates/train.csv')\n",
    "test = pd.read_csv('data/no-duplicates/test.csv')\n",
    "\n",
    "# creating features\n",
    "train['zero-sum'] = (train == 0).sum(axis=1)\n",
    "test['zero-sum'] = (test == 0).sum(axis=1)\n",
    "\n",
    "train['TARGET'] = target\n",
    "\n",
    "# visualize\n",
    "sns.FacetGrid(train, hue=\"TARGET\", size=10) \\\n",
    "   .map(sns.kdeplot, \"zero-sum\") \\\n",
    "   .add_legend()\n",
    "\n",
    "train.pop('TARGET')\n",
    "#train.to_csv('data/no-duplicates/train.csv', index=False)\n",
    "#test.to_csv('data/no-duplicates/test.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us split the zero-sum feature into bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# splitting into bins\n",
    "bins = np.array([0]+range(200,320,10))\n",
    "train['zero-sum-bins'] = np.digitize(train['zero-sum'], bins)\n",
    "test['zero-sum-bins'] = np.digitize(test['zero-sum'], bins)\n",
    "\n",
    "plt.figure(figsize=[18,10])\n",
    "train['TARGET'] = target\n",
    "sns.violinplot(x=\"zero-sum-bins\", y=\"var15\", hue=\"TARGET\", data=train, split=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# loading categorical data\n",
    "cat_train = load_obj('data/categorical/train')\n",
    "cat_test = load_obj('data/categorical/test')\n",
    "\n",
    "# getting new dummies\n",
    "dummies_zs_train = csr_matrix(pd.get_dummies(train['zero-sum-bins'], prefix='zsb', prefix_sep='-'))\n",
    "dummies_zs_test = csr_matrix(pd.get_dummies(test['zero-sum-bins'], prefix='zsb', prefix_sep='-'))\n",
    "\n",
    "# updating\n",
    "cat_train = hstack([cat_train, dummies_zs_train])\n",
    "cat_test = hstack([cat_test, dummies_zs_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## var38\n",
    "\n",
    "var38 has a lot of examples with the same value. This indicates a missing value treatment by the people at Santander. Let us fix it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train['var38-peak'] = np.isclose(train['var38'], 117310.979016)\n",
    "train['var38-log'] = train.loc[~train['var38-peak'], 'var38'].map(np.log)\n",
    "train['var38-peak'] = train['var38-peak'].astype(int)\n",
    "train.loc[train['var38-peak'], 'var38-log'] = 0\n",
    "train.pop('var38')\n",
    "\n",
    "test['var38-peak'] = np.isclose(test['var38'], 117310.979016)\n",
    "test['var38-log'] = test.loc[~test['var38-peak'], 'var38'].map(np.log)\n",
    "test['var38-peak'] = test['var38-peak'].astype(int)\n",
    "test.loc[test['var38-peak'], 'var38-log'] = 0\n",
    "test.pop('var38')\n",
    "\n",
    "# plot\n",
    "sns.FacetGrid(train, hue=\"TARGET\", size=10) \\\n",
    "   .map(sns.kdeplot, \"var38-log\") \\\n",
    "   .add_legend()\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=[18,10])\n",
    "sns.violinplot(x=\"var38-peak\", y=\"var15\", hue=\"TARGET\", data=train, split=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## var3\n",
    "\n",
    "va3 is suspected to be the country of the account holder. Let us explore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c_train = train.var3.value_counts()\n",
    "c_test = test.var3.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print c_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print c_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'number of \"countries\" in train:', len(c_train),'test:', len(c_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us group the uncommon countries under the same categorical variable. So we will end up with 3 categories: 2 (most common), 1 (uncommon) and 0 (error code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "uncommon_train = [i for i in c_train.index if (c_train[i] < 500) and not (i == -999999)]\n",
    "uncommon_test = [i for i in c_test.index if (c_test[i] < 500) and not (i == -999999)]\n",
    "\n",
    "for val in uncommon_train:\n",
    "    replace = train['var3'] == val\n",
    "    train['var3'][replace] = 1\n",
    "\n",
    "for val in uncommon_train + uncommon_test:\n",
    "    replace = test['var3'] == val\n",
    "    test['var3'][replace] = 1\n",
    "\n",
    "# replace error codes\n",
    "val = -999999\n",
    "replace = train['var3'] == val\n",
    "train['var3'][replace] = 0\n",
    "replace = train['var3'] == val\n",
    "train['var3'][replace] = 0\n",
    "    \n",
    "plt.figure(figsize=[15,8])\n",
    "sns.violinplot(x=\"var3\", y=\"var15\", hue=\"TARGET\", data=train, split=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# getting new dummies\n",
    "dummies_var3_train = csr_matrix(pd.get_dummies(train['var3'], prefix='var3', prefix_sep='-'))\n",
    "dummies_var3_test = csr_matrix(pd.get_dummies(test['var3'], prefix='var3', prefix_sep='-'))\n",
    "\n",
    "# updating\n",
    "cat_train = hstack([cat_train, dummies_var3_train])\n",
    "cat_test = hstack([cat_test, dummies_var3_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# removing target column\n",
    "train.pop('TARGET')\n",
    "\n",
    "# real-valued\n",
    "train.to_csv('data/engineered-real/train.csv', index=False)\n",
    "test.to_csv('data/engineered-real/test.csv', index=False)\n",
    "\n",
    "# saving categorical data\n",
    "save_obj(cat_train, 'data/engineered-cat/train')\n",
    "save_obj(cat_test, 'data/engineered-cat/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-linear Age (var15)\n",
    "\n",
    "Age data is non-linear. Putting it in bins could improve result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loading data\n",
    "# train\n",
    "train = pd.read_csv('data/engineered-real/train.csv') \n",
    "test = pd.read_csv('data/engineered-real/test.csv') \n",
    "\n",
    "# test\n",
    "train_cat = load_obj('data/engineered-cat/train')\n",
    "test_cat = load_obj('data/engineered-cat/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add categorical variables to discretize var15\n",
    "# splitting into bins\n",
    "bins = np.array([23, 30, 40, 50, 60, 70])\n",
    "train['var15-bins'] = np.digitize(train['var15'], bins)\n",
    "test['var15-bins'] = np.digitize(test['var15'], bins)\n",
    "\n",
    "plt.figure(figsize=[18,10])\n",
    "train['TARGET'] = target\n",
    "\n",
    "sns.violinplot(x=\"var15-bins\", y=\"zero-sum\", hue=\"TARGET\", data=train, split=True);\n",
    "\n",
    "# plot\n",
    "sns.FacetGrid(train, hue=\"TARGET\", size=10) \\\n",
    "   .map(sns.distplot, \"var15\") \\\n",
    "   .add_legend();\n",
    "\n",
    "# getting new dummies\n",
    "dummies_v15_train = csr_matrix(pd.get_dummies(train[\"var15-bins\"], prefix='v15b', prefix_sep='-'))\n",
    "dummies_v15_test = csr_matrix(pd.get_dummies(test[\"var15-bins\"], prefix='v15b', prefix_sep='-'))\n",
    "\n",
    "# updating\n",
    "train_cat = hstack([train_cat, dummies_v15_train])\n",
    "test_cat = hstack([test_cat, dummies_v15_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similar variables\n",
    "\n",
    "Some variables are expressed as time series. Let us analyse them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# getting linked variables\n",
    "# imp prefix - getting all\n",
    "imp_all = [c for c in train.columns if c.startswith('imp')]\n",
    "\n",
    "# spreading into subsets\n",
    "imp_op = [c for c in imp_all if c.startswith('imp_op')]\n",
    "imp_aport = [c for c in imp_all if c.startswith('imp_aport')]\n",
    "imp_trasp = [c for c in imp_all if c.startswith('imp_trasp')]\n",
    "imp_reemb = [c for c in imp_all if c.startswith('imp_reemb')]\n",
    "imp_compra = [c for c in imp_all if c.startswith('imp_compra')]\n",
    "\n",
    "# updating all\n",
    "imp_all = [c for c in imp_all if c not in imp_op + imp_aport + imp_trasp + imp_reemb + imp_compra]\n",
    "\n",
    "# saldo prefix - getting all\n",
    "saldo = [c.replace('_medio_','_') for c in train.columns if c.startswith('saldo')]\n",
    "\n",
    "# delta\n",
    "delta = [c for c in train.columns if c.startswith('delta')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactions of same variable\n",
    "\n",
    "Suffixes hace e ult indicate time series. Let us explore them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# grouping same variables\n",
    "# will generate new features from these groups\n",
    "var_names = list(set([s[6:min(len(s), find_nth(s,'_',2))] for s in saldo]))\n",
    "\n",
    "saldo_dict ={}\n",
    "for v in var_names:\n",
    "    saldo_dict[v] = target\n",
    "    col_names = [s for s in [c for c in train.columns if c.startswith('saldo')] if v in s]\n",
    "    for c in col_names:\n",
    "        saldo_dict[v] = pd.concat([saldo_dict[v],train[c]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feat_exp = FeatureExpansion()\n",
    "for var in saldo_dict.keys():\n",
    "    df = saldo_dict[var].drop('TARGET', axis=1)\n",
    "    for r in range(6):\n",
    "        df, op_log = feat_exp.fit_transform(df,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Stability selection\n",
    "\n",
    "Too expensive to do on CV. Do it separately and add as a new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# loading data\n",
    "# train\n",
    "train = pd.read_csv('data/engineered-real/train.csv') \n",
    "test = pd.read_csv('data/engineered-real/test.csv') \n",
    "\n",
    "cols = train.columns\n",
    "\n",
    "# joining\n",
    "train = hstack([csr_matrix(train), load_obj('data/engineered-cat/train')]).tocsr()\n",
    "test = hstack([csr_matrix(test), load_obj('data/engineered-cat/test')]).tocsr()\n",
    "\n",
    "p = preprocessing({'na_input': {'strategy': 'mean'}})\n",
    "\n",
    "train = p.fit_transform(train, target)\n",
    "test = p.transform(test)\n",
    "\n",
    "# feature selection algo\n",
    "sel = RandomizedLogisticRegression(sample_fraction=0.50, n_resampling=500, \n",
    "                                   selection_threshold=0.0)\n",
    "# transforming\n",
    "train = sel.fit_transform(train.todense(), np.array(target['TARGET']))\n",
    "test = sel.transform(test.todense())\n",
    "    \n",
    "sel_cols = cols[sel.scores_[0:len(cols)]>0]\n",
    "sel_scores = sel.scores_[sel.scores_[0:len(cols)]>0]\n",
    "\n",
    "print [x for (y,x) in sorted(zip(sel_scores, sel_cols), key=lambda pair: pair[0], reverse=True)] \n",
    "\n",
    "train = csr_matrix(train)\n",
    "test = csr_matrix(test)\n",
    "\n",
    "load_obj('data/selected/st-train')\n",
    "load_obj('data/selected/st-test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%qtconsole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
