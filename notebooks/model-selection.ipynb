{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Santander customer satisfaction: model selection\n",
    "\n",
    "Using hyperopt to define search spaces and optimize hyperparameters. \n",
    "\n",
    "## Imports and code\n",
    "\n",
    "Importing the base code that contains many functions, wrappers for algorithms, cross validation and model selection frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# starting up a console attached to this kernel\n",
    "%matplotlib inline\n",
    "%qtconsole\n",
    "import os\n",
    "\n",
    "# importing base code\n",
    "os.chdir('/home/guilherme/Documents/Kaggle/santander-satisfaction/code')\n",
    "from base import *\n",
    "\n",
    "# changing to competition dir\n",
    "os.chdir('/home/guilherme/Documents/Kaggle/santander-satisfaction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection frameworks and search spaces\n",
    "\n",
    "Fitting algorithms on raw data. Hyperparameter optimization is done using hyperopt (Tree of Parzen Estimators). \n",
    "\n",
    "## XGBoost\n",
    "\n",
    "* **1st-round:** \n",
    "    * less expensive 10-fold CV\n",
    "    * choosing best data \n",
    "    * tweaking class weights\n",
    "    * univariate feature selection\n",
    "    * no feature expansion\n",
    "\n",
    "* **2nd-round:** \n",
    "    * more expensive 7-fold with 5 repetitions CV\n",
    "    * improve on first round\n",
    "\n",
    "* **3rd-round:**\n",
    "    * over and undersampling -> worse! stopped test\n",
    "    * stability selection\n",
    "\n",
    "* **4th-round:**\n",
    "    * focus on feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_number: 1 {'feat_exp': {'n': 0}, 'fit_params': {'eval_metric': 'auc'}, 'preproc': {'var_thres': {'threshold': 0.0}, 'na_input': {'strategy': 'mean'}, 'sel_perc': {'percentile': False, 'score_func': False}}, 'y_transf': None, 'resmpl': {'params': False, 'method': False}, 'params': {'colsample_bytree': 0.48855914089061664, 'scale_pos_weight': 3.089407556793585, 'learning_rate': 0.023696594771697263, 'min_child_weight': 5.0, 'n_estimators': 528.2978498051992, 'subsample': 0.8572278669900759, 'objective': 'binary:logistic', 'max_depth': 5.0, 'gamma': 0.3923056793538462}, 'model': <class 'xgboost.sklearn.XGBClassifier'>, 'data': {'real': 'data/selected/st-train.csv', 'ground-truth': 'data/target.csv', 'cat': None}}\n",
      "cv underway.......round 0 AUC: 0.8373 | average so far: 0.8373 +- 0.0041\n",
      "cv underway"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-dc99e5b94539>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0meval_number\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0mtrials\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrials\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0mbest_xgb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframework\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxgb_space\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrials\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;31m# saving trials\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mbase.py\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(experimental_fmwk, space, max_evals, trials)\u001b[0m\n",
      "\u001b[1;32m/home/guilherme/anaconda2/lib/python2.7/site-packages/hyperopt/fmin.pyc\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, trials, rseed)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m     \u001b[0mrval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFMinIter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malgo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdomain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrials\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_evals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_evals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 334\u001b[1;33m     \u001b[0mrval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    335\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/guilherme/anaconda2/lib/python2.7/site-packages/hyperopt/fmin.pyc\u001b[0m in \u001b[0;36mexhaust\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    292\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 294\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    295\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/guilherme/anaconda2/lib/python2.7/site-packages/hyperopt/fmin.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, N, block_until_done)\u001b[0m\n\u001b[0;32m    266\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m                 \u001b[1;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstopped\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/guilherme/anaconda2/lib/python2.7/site-packages/hyperopt/fmin.pyc\u001b[0m in \u001b[0;36mserial_evaluate\u001b[1;34m(self, N)\u001b[0m\n\u001b[0;32m    185\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m                     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'job exception: %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/guilherme/anaconda2/lib/python2.7/site-packages/hyperopt/fmin.pyc\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[0;32m    112\u001b[0m             pyll_rval = pyll.rec_eval(self.expr, memo=memo,\n\u001b[0;32m    113\u001b[0m                     print_node_on_error=self.rec_eval_print_node_on_error)\n\u001b[1;32m--> 114\u001b[1;33m             \u001b[0mrval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mbase.py\u001b[0m in \u001b[0;36mframework\u001b[1;34m(space)\u001b[0m\n",
      "\u001b[1;32mbase.py\u001b[0m in \u001b[0;36mrun_cv\u001b[1;34m(self, model, X, y, n_reps, fit_params)\u001b[0m\n",
      "\u001b[1;32mbase.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, fit_params)\u001b[0m\n",
      "\u001b[1;32m/home/guilherme/anaconda2/lib/python2.7/site-packages/xgboost/sklearn.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose)\u001b[0m\n\u001b[0;32m    341\u001b[0m                               \u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 343\u001b[1;33m                               verbose_eval=verbose)\n\u001b[0m\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/guilherme/anaconda2/lib/python2.7/site-packages/xgboost/training.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, learning_rates, xgb_model)\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_boost_round\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m             \u001b[0mnboost\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/guilherme/anaconda2/lib/python2.7/site-packages/xgboost/core.pyc\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m    692\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    693\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 694\u001b[1;33m             \u001b[0m_check_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXGBoosterUpdateOneIter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    695\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    696\u001b[0m             \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# search space for hyperparameter optimization\n",
    "xgb_space = {'model': xgb.XGBClassifier,\n",
    "             'params': {'n_estimators' : hp.normal('xgb_n', 500, 100),\n",
    "                        'learning_rate' : hp.uniform('xgb_eta', 0.01, 0.03),\n",
    "                        'max_depth' : hp.quniform('xgb_max_depth', 2, 8, 1),\n",
    "                        'min_child_weight' : hp.quniform('xgb_min_child_weight', 1, 6, 1),\n",
    "                        'subsample' : hp.uniform('xgb_subsample', 0.8, 1),\n",
    "                        'gamma' : hp.uniform('xgb_gamma', 0.0, 0.4),\n",
    "                        'colsample_bytree' : hp.uniform('xgb_colsample_bytree', 0.2, 0.8),\n",
    "                        'objective': hp.choice('xgb_obj', ['binary:logistic']),\n",
    "                        'scale_pos_weight': hp.uniform('xgb_w', 1.0, 4.0)\n",
    "                        },\n",
    "             'preproc': {'na_input': {'strategy': 'mean'},\n",
    "                         'var_thres': {'threshold': 0.0},\n",
    "                         'sel_perc': {'score_func': False,\n",
    "                                      'percentile': False}\n",
    "                        },\n",
    "             'resmpl': hp.choice('resmpl', [{'method': False, 'params': False}]),\n",
    "             'data': hp.choice('dc',[{'real': 'data/selected/st-train.csv',\n",
    "                                      'cat': None, \n",
    "                                      'ground-truth': 'data/target.csv'}]),                \n",
    "             'feat_exp': {'n': 0}, #hp.quniform('exp_n', 0, 100, 20)\n",
    "             'fit_params': {'eval_metric': 'auc'},\n",
    "             'y_transf': hp.choice('trf', [None]),\n",
    "            }\n",
    "\n",
    "# model selection\n",
    "eval_number = 0\n",
    "trials = Trials()      \n",
    "best_xgb = optimize(framework, xgb_space, 20, trials)\n",
    "\n",
    "# saving trials\n",
    "save_obj(trials, 'trials/4th-round')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit Analysis\n",
    "\n",
    "Checking evolution of trials, residuals and overall fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# loading trials\n",
    "trials = load_obj('trials/3rd-round')\n",
    "\n",
    "# reading R-squared\n",
    "r2 = [trials.trials[i]['result']['auc_avg'] for i in range(len(trials.trials))]\n",
    "r2_std = [trials.trials[i]['result']['auc_std'] for i in range(len(trials.trials))]\n",
    "\n",
    "# plotting trials results\n",
    "plt.figure(figsize=[15,10])\n",
    "plt.plot(range(len(r2)), r2, 'ko')\n",
    "plt.xlabel('Trial number')\n",
    "plt.ylabel('AUC')\n",
    "plt.errorbar(range(len(r2)), r2, yerr=r2_std, fmt='ko')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorted trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# loading trials\n",
    "trials = load_obj('trials/3rd-round')\n",
    "\n",
    "# getting top 10 trials\n",
    "top = [get_best(trials, i) for i in range(len(trials.trials))]\n",
    "\n",
    "r2 = [top[i]['result']['auc_avg'] for i in range(len(top))]\n",
    "r2_std = [top[i]['result']['auc_std'] for i in range(len(top))]\n",
    "\n",
    "# plotting trials results\n",
    "plt.figure(figsize=[15,10])\n",
    "plt.plot(range(len(r2)), r2, 'ko')\n",
    "plt.axis([-1, len(trials.trials), 0.76, 0.86])\n",
    "plt.errorbar(range(len(r2)), r2, yerr=r2_std, fmt='ko')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter importance\n",
    "\n",
    "Let us plot the influence of each hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# loading trials\n",
    "import seaborn as sns\n",
    "trials = load_obj('trials/3rd-round')\n",
    "top = [get_best(trials, i) for i in range(len(trials.trials))]\n",
    "result = [top[i]['result']['auc_avg'] for i in range(len(top))]\n",
    "\n",
    "hyper_df = hypersummary(top, result, 'auc')\n",
    "\n",
    "hyper_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = sns.PairGrid(hyper_df, hue='auc')\n",
    "g.map_diag(plt.hist)\n",
    "g.map_offdiag(plt.scatter)\n",
    "g.savefig('vis/hyperparameters/2nd-round.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curves\n",
    "\n",
    "Let us check the results in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# loading trials\n",
    "trials = load_obj('trials/3rd-round')\n",
    "top = [get_best(trials, i) for i in range(len(trials.trials))]\n",
    "\n",
    "# ranges\n",
    "rngs = get_plot_ranges(1, 40)\n",
    "    \n",
    "# plot ROC for the best models\n",
    "for i in range(len(rngs)):\n",
    "    \n",
    "    plt.figure(figsize=[18,10])\n",
    "    count = 1\n",
    "    for model in rngs[i]:\n",
    "    \n",
    "        # getting ith best model (0 is the best)\n",
    "        space = get_best(trials, ind=model)\n",
    "        plt.subplot(1, 1, count)\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.title('Model {0} (mean AUC = {1:.3f}) '.format(model, space['result']['auc_avg']))\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "\n",
    "        for fold in range(10):\n",
    "\n",
    "            # computing roc\n",
    "            probs = space['result']['results']['folds']['probs'][fold]\n",
    "            gnd_truth = space['result']['results']['folds']['gnd_truth'][fold]\n",
    "            fpr, tpr, thresholds = metrics.roc_curve(gnd_truth, probs)\n",
    "            auc = metrics.roc_auc_score(gnd_truth, probs) \n",
    "                \n",
    "            # plotting\n",
    "            plt.plot(fpr, tpr, label='Fold {0} (area = {1:.3f})'.format(fold, auc))\n",
    "            plt.legend(loc=\"lower right\")\n",
    "        \n",
    "        count += 1\n",
    "    \n",
    "    # saving full plot\n",
    "    directory = 'vis/roc_curves/2nd-round'\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    print directory + '/roc-{}.png'.format(i)\n",
    "    plt.savefig(directory + '/roc-{}.png'.format(i))\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembling\n",
    "\n",
    "Can we get better performance if we average model predictions?\n",
    "\n",
    "Loading trials and model data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loading trials\n",
    "trials = load_obj('trials/3rd-round')\n",
    "trials = [get_best(trials, i) for i in range(len(trials.trials))]\n",
    "\n",
    "# extracting target\n",
    "target_m = []\n",
    "for r in range(5):\n",
    "    fold_m = []\n",
    "    for f in range(7):\n",
    "        fold_m.append(np.array(trials[0]['result']['results'][r]['folds']['gnd_truth'][f]))\n",
    "    target_m.append(fold_m)\n",
    "# as numpy array\n",
    "target_m = np.array(target_m)\n",
    "\n",
    "# extracting model data\n",
    "model_dict = {}\n",
    "for m in range(40):\n",
    "    model_dict[m] = {}\n",
    "    probs_m = []\n",
    "    auc = []\n",
    "    for r in range(5):\n",
    "        fold_m = []\n",
    "        for f in range(7):\n",
    "             fold_m.append(np.array(trials[m]['result']['results'][r]['folds']['probs'][f]))\n",
    "        probs_m.append(fold_m)\n",
    "        \n",
    "    model_dict[m]['pm'] = probs_m\n",
    "    model_dict[m]['auc'] = trials[m]['result']['auc_avg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_7_5_fold(target_m, models, weights=None):\n",
    "    probs = []\n",
    "    for r in range(5):\n",
    "        folds = []\n",
    "        for f in range (7):\n",
    "            folds.append([0]*len(target_m[r][f]))\n",
    "        probs.append(folds)\n",
    "    \n",
    "    for i, model in enumerate(models):\n",
    "        for r in range(5):\n",
    "            for f in range (7):\n",
    "                if weights == None:\n",
    "                    probs[r][f] = probs[r][f] + model['pm'][r][f]*1/len(models)\n",
    "                else:\n",
    "                    probs[r][f] = probs[r][f] + model['pm'][r][f]*weights[i]/sum(weights)\n",
    "    aucs = []\n",
    "    for r in range(5):\n",
    "        folds = []\n",
    "        for f in range(7):\n",
    "            folds.append(metrics.roc_auc_score(target_m[r][f], probs[r][f]))\n",
    "        best = folds.index(max(folds))\n",
    "        worst = folds.index(min(folds))\n",
    "        folds = [v for i, v in enumerate(folds) if i not in [best, worst]]\n",
    "        aucs.append(folds)\n",
    "    return(np.mean(aucs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple average: greedy\n",
    "\n",
    "Joining results of best models in a greedy manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "in_models = []\n",
    "model_inds = []\n",
    "max_score = 0\n",
    "for m in range(40):\n",
    "    \n",
    "    score = eval_7_5_fold(target_m, in_models + [model_dict[m]])\n",
    "    if score > max_score:\n",
    "        max_score = score\n",
    "        in_models.append(model_dict[m])\n",
    "        model_inds.append(m)\n",
    "        print 'AUC {0:.6f} for combination: {1}'.format(score, model_inds)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importances\n",
    "\n",
    "Checking which ones are more important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions\n",
    "\n",
    "Predicting on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ens_preds = []\n",
    "for i in [0, 1, 2, 4, 5, 10]:    \n",
    "    \n",
    "    # getting space\n",
    "    trials = load_obj('trials/2nd-round')\n",
    "    space = get_best(trials, i)['result']['parameters']\n",
    "\n",
    "    # reading csv & casting into hashable type\n",
    "    train_real = pd.read_csv(space['data']['real'])\n",
    "    y_train = pd.read_csv(space['data']['ground-truth'])['TARGET']\n",
    "\n",
    "    # same for test\n",
    "    test_real = pd.read_csv(space['data']['real'].replace('train', 'test'))\n",
    "\n",
    "    # feature expansion\n",
    "    feat_exp = FeatureExpansion()\n",
    "    train_real = feat_exp.transform(train_real, space['feat_exp']['op_log'])\n",
    "    train_real = csr_matrix(train_real)\n",
    "    test_real = feat_exp.transform(test_real, space['feat_exp']['op_log'])\n",
    "    test_real = csr_matrix(test_real)\n",
    "    \n",
    "    # categorical data\n",
    "    try:\n",
    "        # train\n",
    "        train_cat = load_obj(space['data']['cat'])\n",
    "        train = hstack([train_real, train_cat]).tocsr()\n",
    "        # test\n",
    "        test_cat = load_obj(space['data']['cat'].replace('train', 'test'))\n",
    "        test = hstack([test_real, test_cat]).tocsr()\n",
    "    except:\n",
    "        train = train_real.tocsr()\n",
    "        test = test_real.tocsr()\n",
    "\n",
    "    # casting values to int\n",
    "    space['params']['n_estimators'] = int(space['params']['n_estimators'])\n",
    "    space['preproc']['sel_perc']['percentile'] = int(space['preproc']['sel_perc']['percentile'])\n",
    "\n",
    "    # model loaded in search space\n",
    "    algo = space['model'](**space['params'])\n",
    "    y_transform = TargetTransform(space['y_transf'])\n",
    "    model = sklearn_wrapper(algo, space['preproc'], y_transform)\n",
    "\n",
    "    # fitting and predicting\n",
    "    model.fit(train, y_train, fit_params={'eval_metric': 'auc'})\n",
    "    probs = model.predict_proba(test)\n",
    "    ens_preds.append(probs[:,1])\n",
    "    sys.stdout.write('{}, '.format(i))\n",
    "    \n",
    "# ids\n",
    "ID_col = pd.read_csv('data/raw/test.csv')['ID']\n",
    "\n",
    "# averaging predictions\n",
    "ens_preds = np.mean(ens_preds, axis=0)\n",
    "#weights = np.array([0.19339375, 0.92619747, 0.01661305,  0.39647386,  0.49869799,  0.31282152])\n",
    "#new_preds = np.transpose(np.array(ens_preds)) * np.transpose(weights)\n",
    "#new_preds = np.sum(new_preds, axis=1)/sum(weights)\n",
    "\n",
    "# making a submission\n",
    "sub = pd.DataFrame({'ID': np.array(ID_col).astype(int), 'TARGET': ens_preds})\n",
    "sub.to_csv('submissions/sub11.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission log\n",
    "\n",
    "* **sub7:** single xgb, 1st round best - CV: 0.842 | LB: 0.840010.\n",
    "* **sub8:** ensemble of 20 best models of 1st round - CV: ? | LB: 0.841072.\n",
    "* **sub9:** best model of 2nd round - CV: 0.842240 | LB: 0.840917\n",
    "* **sub10:** greedy optimization ensemble of 2nd round models (output average) - CV: 0.842503 | LB: 0.841378\n",
    "* **sub11:** optimization of weights of ensemble of 2nd round models (output weighted average) - CV: 0.842525 | LB:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Rules of thumb\n",
    "\n",
    "Some rules of thumb can be added in the submission, such as everyone under age 23 is happy. Let us see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = pd.read_csv('submissions/sub10.csv')['TARGET']\n",
    "tc = pd.read_csv('data/no-duplicates/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nv = tc['num_var33'] + tc['saldo_medio_var33_ult3'] + tc['saldo_medio_var44_hace2'] + tc['saldo_medio_var44_hace3'] + tc['saldo_medio_var33_ult1'] + tc['saldo_medio_var44_ult1']\n",
    "\n",
    "preds[nv > 0][1] = 0\n",
    "preds[tc['var15'] < 23] = 0\n",
    "preds[tc['saldo_medio_var5_hace2'] > 160000] = 0\n",
    "preds[tc['saldo_var33'] > 0] = 0\n",
    "preds[tc['var38'] > 3988596] = 0\n",
    "preds[tc['var21'] > 7500] = 0\n",
    "preds[tc['num_var30'] > 9] = 0\n",
    "preds[tc['num_var13_0'] > 6] = 0\n",
    "preds[tc['num_var33_0'] > 0] = 0\n",
    "preds[tc['imp_ent_var16_ult1'] > 51003] = 0\n",
    "preds[tc['imp_op_var39_comer_ult3'] > 13184] = 0\n",
    "preds[tc['saldo_medio_var5_ult3'] > 108251] = 0\n",
    "\n",
    "ID_col = tc['ID']\n",
    "\n",
    "# making a submission\n",
    "sub = pd.DataFrame({'ID': np.array(ID_col).astype(int), 'TARGET': preds})\n",
    "sub.to_csv('submissions/sub12.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
